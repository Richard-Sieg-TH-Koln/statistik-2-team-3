{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passt für sentiment analysis\n",
    "import pandas as pd\n",
    "from germansentiment import SentimentModel\n",
    "# cuda version\n",
    "# import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: datasets in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (2022.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (1.4.4)\n",
      "Requirement already satisfied: dill in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (1.21.5)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (0.70.14)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (2.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.0 responses-0.18.0\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install scikit-learn\n",
    "# !pip install evil accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting germansentiment\n",
      "  Downloading germansentiment-1.1.0-py3-none-any.whl (5.8 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 19.9 MB/s eta 0:00:00\n",
      "Collecting torch>=1.8.1\n",
      "  Downloading torch-2.0.1-cp39-cp39-win_amd64.whl (172.4 MB)\n",
      "     -------------------------------------- 172.4/172.4 MB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch>=1.8.1->germansentiment) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch>=1.8.1->germansentiment) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch>=1.8.1->germansentiment) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch>=1.8.1->germansentiment) (4.3.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch>=1.8.1->germansentiment) (2.8.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers->germansentiment) (2022.7.9)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-win_amd64.whl (263 kB)\n",
      "     -------------------------------------- 263.9/263.9 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "     -------------------------------------- 236.8/236.8 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers->germansentiment) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers->germansentiment) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 15.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers->germansentiment) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers->germansentiment) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers->germansentiment) (1.21.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->germansentiment) (2022.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers->germansentiment) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers->germansentiment) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.1->germansentiment) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers->germansentiment) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers->germansentiment) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers->germansentiment) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers->germansentiment) (2022.9.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from sympy->torch>=1.8.1->germansentiment) (1.2.1)\n",
      "Installing collected packages: tokenizers, safetensors, torch, huggingface-hub, transformers, germansentiment\n",
      "Successfully installed germansentiment-1.1.0 huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 torch-2.0.1 transformers-4.30.2\n",
      "Requirement already satisfied: torch in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install germansentiment\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../data/songs_complete_final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp39-cp39-win_amd64.whl (4.9 MB)\n",
      "     ---------------------------------------- 4.9/4.9 MB 13.1 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-2.0.2%2Bcu117-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 15.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.0.2+cu117 torchvision-0.15.2+cu117\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PyTorch is not linked with support for mps devices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2116\\3975907802.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmps_device\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mps\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmps_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"mps\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1900\u001b[0m             )\n\u001b[0;32m   1901\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1143\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1145\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 820\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m-> 1143\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PyTorch is not linked with support for mps devices"
     ]
    }
   ],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "model.model.to(mps_device)\n",
    "model.device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, probabilities = model.predict_sentiment(dataset[\"lyrics\"][:10], output_probabilities = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('negative',\n",
      "  [['positive', 0.012294579297304153],\n",
      "   ['negative', 0.9867066740989685],\n",
      "   ['neutral', 0.0009987399680539966]],\n",
      "  'Powpow  Dicka, das kein Rap mehr, das ist Kindergarten Ihr wollt euch '\n",
      "  'treffen, doch dann lass ich eure Mütter warten Es ist KUKU TEAM, bringe '\n",
      "  'euch gemischte Karten Ihr könnt nicht rappen, dafür könnt ihr\\u205fso\\u205f'\n",
      "  'wie\\u205fBitches blasen Ich geb\\u205fdem Bastard jetzt\\u205feinn '\n",
      "  'Gnadenschuss Ihr seid keine Straßenjungs, ihr färbt euch die Haare bunt '\n",
      "  'Hör, mein Magen knurrt, Köfte im Fladenbrot Mach ein Nickerchen auf Sofa so '\n",
      "  'wie Baba Packs, Waage, Flex, Dicka, dis kein StanniKurs Was ist das für '\n",
      "  'Blick Moruk, als ob deine Mami furzt Heiße Aura wie Solarium Besser '\n",
      "  'verschwinde aus mein Radius Immer gute Lage so wie Mehringdamm Bares über '\n",
      "  'Telegram, Ware wie ein Chemiker Ihr Huren seid mir scheißegal Ob legal, '\n",
      "  'illegal, ist mir scheißegal'),\n",
      " ('negative',\n",
      "  [['positive', 0.012251143343746662],\n",
      "   ['negative', 0.9760991930961609],\n",
      "   ['neutral', 0.011649779044091702]],\n",
      "  'Gefährliche, gefährliche KiKiKiKi Gefährliche KiKiKiKi Gefährliche KiKiKiKi '\n",
      "  'Gefährliche KiKiKiKi Kobra Militär Ja, ja, ja, gib ihm  Du weißt doch, '\n",
      "  'Jubel kommt erst, wenn es klappt ja Sie warten drauf, dass ich verkack ja '\n",
      "  'Trubel im Kopf jeden Tag ja Es bedeutet nicht viel, wenn ich lach nein Es '\n",
      "  'bedeutet erst viel, wenn ich mach ja Erfolg entsteht nicht über Nacht nein '\n",
      "  'Jubel kommt erst, wenn es klappt ja Vorher Kugeln auf mich wie auf Pac  '\n",
      "  'Werf kein Auge auf mich, besser schließ sie Hol dir Feuer und zünde an, '\n",
      "  'easy Die schwarzen Wolken kommn nicht vom dem Weezy Mein Film ist viel zu '\n",
      "  'doll, ruf nach Scorsese Oder Tarantino, Vater war Rapstar mit MelodieFlows '\n",
      "  'Wie Kennedy tot, Memory droht, cest la vie Not Sie war erst sechs und ihr '\n",
      "  'Enemy rot Diablo Das Leben will mich zum Idiot zum Idiot Insta sagt, alles '\n",
      "  'im Lot jaja Die Hater sagen, sie fliegt hoch nein, nein, nein Aber niemals, '\n",
      "  'ich kämpfe hier nur für mein Brot Suche den richtigen Ton Nur für einn '\n",
      "  'mickrigen, mickrigen Lohn no Verticke Geschichten im Rohen oh no Lasse die '\n",
      "  'Augen doch ruhen Ich lasse nicht ab von dem Thron'),\n",
      " ('neutral',\n",
      "  [['positive', 0.005406889598816633],\n",
      "   ['negative', 0.020907728001475334],\n",
      "   ['neutral', 0.9736853837966919]],\n",
      "  'Aus meiner Stadt fliegen Leuchtclips und Breslauer Kann sein, dass du '\n",
      "  'enttäuscht bist, wenn ich dir ein deutliches Brett haue Zwanzig Jahre '\n",
      "  'Terror aus der Mainstadt UF, zwanzig Jahre sterben für die Eintracht '\n",
      "  'Zwanzig Jahre Moseleck, zwanzig Jahre Drogengeld Frag deinn Vadder ma, seit '\n",
      "  'zwanzig Jahren wird hier so gerappt Denn ich bin Grönemeyer für die '\n",
      "  'Streetchabos Ich hab schöne Eier und ich lieb Chaos Sag, wie heißt die '\n",
      "  'Stadt, wo du verlierst, Alder Die Stadt, in der wir alles dominiern, Alder '\n",
      "  'Die Stadt, in der die Banken kontrolliern, Alder Hier blickst du bei Action '\n",
      "  'in die Waffe und du stirbst, Alder Zwanzig Jahre Trommeln auf dem Asphalt '\n",
      "  'Zwanzig Jahre jagen wir die Schmocks schon durch den Stadtwald Nenn mich '\n",
      "  'Nike, wenn du angeknockt schwächelst Verpasst man dir einn Gong, dass deine '\n",
      "  'Wangenknochen brechen  Seit zwanzig Jahrn gibt es heut den Adler mit dem '\n",
      "  'Kranz am Hals Und stecken immer noch dem Rest der Erde unsern Schwanz inn '\n",
      "  'Spalt In dieser Stadt, wo die Gewalt zum guten Ton gehört Und man die Diva '\n",
      "  'liebt, von Hessens Norden bis zum OpelWerk Ich hab gelitten und gekämpft '\n",
      "  'für diese Gruppe Mein Herz und meine Leidenschaft geschenkt für diese '\n",
      "  'Gruppe Fünf Jahre Stadionverbot erlebt in dieser Gruppe Mit paar Brüdern, '\n",
      "  'die dich killn, also rede nicht, du Fotze Ich bin ein Kurvenkind, die halbe '\n",
      "  'Welt bereist mit euch Meist abgefuckt und eingezäunt, doch niemals diese '\n",
      "  'Zeit bereut Ich bleibe Ultra und kein BallermannIdiot Und meine Crew bleibt '\n",
      "  'Nummer eins von Alemania bis zum Mond'),\n",
      " ('negative',\n",
      "  [['positive', 0.013118092902004719],\n",
      "   ['negative', 0.9868316650390625],\n",
      "   ['neutral', 5.0243765144841745e-05]],\n",
      "  'Because youre so sweet You lift up my heart And Ill fall back again But I '\n",
      "  'dont know Quite\\u2005which\\u2005way to turn  The\\u2005light in your eyes '\n",
      "  'Touches the sky And\\u2005Im your man again And you hold me As close as '\n",
      "  'close\\u205fcan\\u205fbe  And\\u205fmanys the time That\\u205fyouve enchanted '\n",
      "  'me I\\u205fwant you back Here in my loving arms For eternity   You look and '\n",
      "  'you smile Warm me inside Cause youre so good to me A beauty Filled with the '\n",
      "  'joys of spring'),\n",
      " ('neutral',\n",
      "  [['positive', 0.02248387783765793],\n",
      "   ['negative', 0.028418434783816338],\n",
      "   ['neutral', 0.9490976333618164]],\n",
      "  '. Liquid Swords  GZA  Actual . souljaboytellem.com  Soulja Boy  . Only '\n",
      "  'Built  Cuban Linx  Raekwon  . Enter the WuTang  Chambers  WuTang Clan  . '\n",
      "  'Death Certificate  Ice Cube  . ATLiens  OutKast  . The Infamous  Mobb Deep  '\n",
      "  '. Doggystyle  Snoop Dogg  . Illmatic  Nas  . Ready to Die  Biggie Smalls  . '\n",
      "  'Stress The Extinction Agenda  Organized Konfusion  . Aquemini  OutKast  . '\n",
      "  'The Low End Theory  A Tribe Called Quest  . Whut Thee Album  Redman  . '\n",
      "  'Live  Let Die  Kool G Rap  DJ Polo  . It Takes a Nation of Millions to Hold '\n",
      "  'Us Back  Public Enemy  . Madvillainy  Madvillain  . AmeriKKKas Most Wanted  '\n",
      "  'Ice Cube  . Ironman  Ghostface Killah  . The Sun Rises in the East  Jeru '\n",
      "  'the Damaja  .  Midnight Marauders  A Tribe Called Quest  . ,,  Kool G Rap  '\n",
      "  '. Resurrection  Common  . The Chronic  Dr. Dre  . The Diary  Scarface  . '\n",
      "  'Reasonable Doubt  JayZ  . Bizarre Ride II The Pharcyde  The Pharcyde  . '\n",
      "  'Capital Punishment  Big Pun  . The Eminem Show  Eminem  . Fear of a Black '\n",
      "  'Planet  Public Enemy  . Supreme Clientele  Ghostface Killah  . Mos Def  '\n",
      "  'Talib Kweli are Black Star  Black Star  . Lifestylez ov da Poor  Dangerous  '\n",
      "  'Big L  . Paid in Full  Eric B.  Rakim  . Me Against the World  Pac  . To '\n",
      "  'Pimp a Butterfly  Kendrick Lamar  . Dare iz a Darkside  Redman  . '\n",
      "  'Southernplayalisticadillacmuzik  OutKast  . The College Dropout  Kanye '\n",
      "  'West  . Illadelph Halflife  The Roots  . Like Water for Chocolate  Common  '\n",
      "  '. My Beautiful Dark Twisted Fantasy  Kanye West  . Hell on Earth  Mobb '\n",
      "  'Deep  . Its Dark and Hell is Hot  DMX  . The Black Album  JayZ  . The '\n",
      "  'Marshall Mathers LP  Eminem  . WordLife   O.C.  . Moment of Truth  Gang '\n",
      "  'Starr  . Breaking Atoms  Main Source  . Goodfellas  Showbiz  A.G.  . Enta '\n",
      "  'Da Stage  Black Moon'),\n",
      " ('negative',\n",
      "  [['positive', 0.048646487295627594],\n",
      "   ['negative', 0.9505898952484131],\n",
      "   ['neutral', 0.0007636932423338294]],\n",
      "  'Yo, its that boy Denzel Curry Im finna do this for my niggas, mane Its the '\n",
      "  'intro, nigga I hope yall enjoy, it nigga, ya know what Im saying This the '\n",
      "  'King Remembered Underground Tape  to motherfuckin , my nigga Raider Klan is '\n",
      "  'in this bitch, shoutout to SpaceGhostPurrp, my nigga Yall better like... '\n",
      "  'yall better support a nigga, bruh Dat, yeee'),\n",
      " ('negative',\n",
      "  [['positive', 0.02663077414035797],\n",
      "   ['negative', 0.5013036131858826],\n",
      "   ['neutral', 0.47206565737724304]],\n",
      "  'Kippe Mischen und verliere meine Daten Steuerung und S das was gut ist hab '\n",
      "  'ich safe Kaufe Gin und ein paar Mischen formatieren meine Datenträger Poppe '\n",
      "  'Bottles und ich fühl mich wie ein Kind Keine Fehler weil es neu ist mache '\n",
      "  'Fehler weil ich will Wenn ich will krieg ich alles was ich will Aber fühl '\n",
      "  'mich dabei leer schalte ab und leg mich hin Tausеnd Menschen und ich fühle '\n",
      "  'mich allеine ja Ich will mit dem System interagieren finde nur keine '\n",
      "  'Treiber ja Heute geh ich schwimmen in der Sonne morgen sinke ich im tiefen '\n",
      "  'Wasser Ich bin heut ein guter Junge aber morgen dann ein ziemlich krasser '\n",
      "  'Motherfucker Heute bin ich da für die Welt morgen halte ich von allem '\n",
      "  'Abstand Weiß nie wann es reicht ich bin müde doch entscheide mich für noch '\n",
      "  'nen Upper Ich bin völlig unter Strom hab nen Lauf Dann kontakt mit nem '\n",
      "  'Jibbet RCDs lösen aus Sie will ins Bett auf Acid geh zwar darauf ein doch '\n",
      "  'bin nicht down Sehe Fabelwesen zieh mich wieder an und renne raus ja Ich '\n",
      "  'verwerfe meine Pläne leb im Traum Und mein Papierkorb leert sich pünktlich '\n",
      "  'alle  Tage aus Copy paste jeden Tag Emotionen sind nicht da Und ich fahre '\n",
      "  'viel zu schnell durch jede Kurve ist mir völlig egal Heute geh ich '\n",
      "  'schwimmen in der Sonne morgen sinke ich im tiefen Wasser Ich bin heut ein '\n",
      "  'guter Junge aber morgen dann ein ziemlich krasser Motherfucker Heute bin '\n",
      "  'ich da für die Welt morgen halte ich von allem Abstand Weiß nie wann es '\n",
      "  'reicht ich bin müde doch entscheide mich für noch nen Upper'),\n",
      " ('negative',\n",
      "  [['positive', 0.020809665322303772],\n",
      "   ['negative', 0.9790260195732117],\n",
      "   ['neutral', 0.0001643591094762087]],\n",
      "  'Yo pls dont call me Im busy at the moment U may call the receptionist He '\n",
      "  'will pass\\u2005the\\u2005call to me Chorus Started\\u2005from the bottom now '\n",
      "  'we here All\\u2005my drip is invested in my head Lost my life now\\u205f'\n",
      "  'Im\\u205ftryna\\u205fget it back Who\\u205fare u u\\u205flook like u drive a '\n",
      "  'van Pretty deep cause you know Im kinda black  we were all oppressed '\n",
      "  'Reminiscing alot of things about the past Money heist Im the man behind the '\n",
      "  'mask Verse Its a long journey of my life Long trips tryna bite now I show I '\n",
      "  'get paid No please dont pretend Im proud cause Im self made I survive when '\n",
      "  'they set traps Long shots on my lane No please dont pretend Repeat when '\n",
      "  'they play mine Press skip when they play urs Drip drip nangutatakho No trip '\n",
      "  'Nangu Yanga U gon get shot When u trespass Do u get that Do u dig that Do u '\n",
      "  'know that on my part'),\n",
      " ('negative',\n",
      "  [['positive', 0.39600318670272827],\n",
      "   ['negative', 0.583101749420166],\n",
      "   ['neutral', 0.020895056426525116]],\n",
      "  'yýlýnda Refahyol hükümetinin iki liderine taþlama olarak yazýlmýþtýr... Bu '\n",
      "  'yoksulluk beni delledi Aldý aklým yaktý külledi Bunu yapan iki kiþi Biri '\n",
      "  'erkek biri diþi Halden bilmez iki kiþi Vay.. Çilli kedi, çilli kedi vay '\n",
      "  'Ciðerimi yedin kedi vay Bunu yapan iki kiþi Biri erkek biri diþi Halden '\n",
      "  'bilmez iki kiþi Vay bee..'),\n",
      " ('neutral',\n",
      "  [['positive', 0.03788045793771744],\n",
      "   ['negative', 0.04872845485806465],\n",
      "   ['neutral', 0.91339111328125]],\n",
      "  'Studio AlbumsThe Rolling Stones , UK  Englands Newest Hit Makers , US  X  , '\n",
      "  'US The Rolling Stones No.  , UK  The Rolling Stones, Now , US Out of Our '\n",
      "  'Heads  Decembers Children And Everybodys , US Aftermath Between the '\n",
      "  'Buttons  Their Satanic Majesties Request  Beggars Banquet  Let It Bleed  '\n",
      "  'Sticky Fingers  Exile on Main St  Goats Head Soup  Its Only Rock n Roll  '\n",
      "  'Black and Blue  Some Girls  Emotional Rescue  Tattoo You  Undercover  Dirty '\n",
      "  'Work  Steel Wheels  Voodoo Lounge  Bridges to Babylon  A Bigger Bang  Blue  '\n",
      "  'Lonesome Live albums \\tGot Live If You Want It  \\tGet Yer YaYas Out The '\n",
      "  'Rolling Stones in Concert  \\tLove You Live  \\tStill Life  \\tFlashpoint  \\t'\n",
      "  'Stripped  \\tThe Rolling Stones Rock and Roll Circus  \\tNo Security  \\tLive '\n",
      "  'Licks  \\tShine a Light  \\tBrussels Affair Live   Some Girls Live in '\n",
      "  'Texas   \\tHampton Coliseum Live   L.A. Friday Live   Live at the '\n",
      "  'Checkerboard Lounge, Chicago   Live at the Tokyo Dome Live   Light the Fuse '\n",
      "  'Live   Live at Leeds Live   Sweet Summer Sun  Marquee Club Live   Sticky '\n",
      "  'Fingers Live  Havana MoonCompilation albums \\tBig Hits High Tide and Green '\n",
      "  'Grass  \\tFlowers  \\tThrough the Past, Darkly Big Hits Vol.   \\tStone Age  '\n",
      "  'Gimme Shelter  Hot Rocks   \\tMilestones  Rock n Rolling Stones  More Hot '\n",
      "  'Rocks Big Hits  Fazed Cookies  \\tNo Stone Unturned  \\tMetamorphosis   Made '\n",
      "  'in the Shade  Rolled Gold The Very Best of the Rolling Stones  \\tGet '\n",
      "  'Stoned  Greatest Hits  \\tTime Waits for No One  \\tSolid Rock  \\tSlow '\n",
      "  'Rollers  \\tSucking in the Seventies  \\tStory of The Stones  \\tRewind   \\t'\n",
      "  'Singles Collection The London Years  \\tJump Back The Best of The Rolling '\n",
      "  'Stones  \\tForty Licks  \\tRarities   \\tGRRR  \\tNo Stone Unturned Vol.   Box '\n",
      "  'setsOriginal Master Recordings  The Rolling Stones   The Rolling Stones   '\n",
      "  'You Get What You Need  The Rolling Stones In Mono Special reissuesSome '\n",
      "  'Girls  Exile On Main St.')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(zip(classes, probabilities, dataset[\"lyrics\"][:10])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, ..., 1, 1, 1], dtype=int64), Index(['Rock', 'Rap', 'Pop', 'Schlager'], dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ds = pd.read_csv(\"../data/songs_complete_final.csv\")\n",
    "# große genres\n",
    "ds.dropna(subset=[\"genre_cat\"], inplace=True)\n",
    "ds[\"genre_cat\"].unique()\n",
    "# label sollen in Zahlen umgewandelt werden\n",
    "factorized = pd.factorize(ds[\"genre_cat\"])\n",
    "print(factorized)\n",
    "ds[\"labels\"] = factorized[0]\n",
    "ds.to_csv(\"../data/processed/songs_complete_final_no_nan.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/linh nguyen/.cache/huggingface/datasets/csv/default-04a225a5d88f88c9/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b2b8670ba24ecbadf521cf4d1e41d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8749055c7948f1891729be960308aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919d88f07e464a5ead428a8a89097dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/linh nguyen/.cache/huggingface/datasets/csv/default-04a225a5d88f88c9/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7a9b5ef46c4b33af40e92c46390ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['artist', 'artist_id', 'album', 'album_id', 'release_date', 'title', 'full_title', 'song_id', 'lyrics', 'release_year', 'weekday', 'genre', 'genre_cat', 'word_count', 'labels'],\n",
      "        num_rows: 16429\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "hf_dataset = load_dataset(\"csv\", data_files=\"../data/processed/songs_complete_final_no_nan.csv\")\n",
    "# hf.dataset = Dataset.form_pandas(ds) --> am anfang reinladen\n",
    "# hf_dataset=hf_dataset.rename_columns({\"genre_cat\": \"labels\"})\n",
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13143\n",
      "3286\n"
     ]
    }
   ],
   "source": [
    "split = hf_dataset[\"train\"].train_test_split(test_size=0.2) # weil auf csv reingeladen wurde\n",
    "# split = hf_dataset.train_test_split(test_size=0.2) # auf 20% der Daten\n",
    "# unterschied validation set und test set: wenn man zu wenig Daten hat\n",
    "# validation für hyperparameter, brauchen wir in unserem Fall nicht\n",
    "train_ds = split[\"train\"]\n",
    "test_ds = split[\"test\"]\n",
    "print(len(train_ds))\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruft tokenizer auf die Lyrics aus\n",
    "# NN erwartet überall gleiche Länge --> mit padding auffüllen und truncation abschneiden --> gleiche Länge\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"lyrics\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae22e2cb3f1c48bb92d43947b3a350d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd2400feeaa4e1ea0eeb795e4009b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenized = train_ds.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = train_tokenized.shuffle(seed=42).select(range(1000)) # auf 1000 trainieren\n",
    "small_test_dataset = test_tokenized.shuffle(seed=42).select(range(100)) # auf 100 testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Rock', 1: 'Rap', 2: 'Pop', 3: 'Schlager'}\n"
     ]
    }
   ],
   "source": [
    "# damit es human readable ist\n",
    "id2label = {idx: val for idx, val in enumerate(factorized[1])}\n",
    "print(id2label)\n",
    "label2id = {val: key for (key, val) in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=4, id2label=id2label, label2id=label2id)\n",
    "# model.to(\"mps\")\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.21.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.15.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.28.1)\n",
      "Requirement already satisfied: accelerate>=0.20.2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\n",
      "Requirement already satisfied: sympy in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (1.26.11)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\linh nguyen\\anaconda3\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_genre\",\n",
    "    # use_mps_device=True,\n",
    "    num_train_epochs=2,\n",
    "    # learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a877add0844fc3b7cc20c7d1842e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7d0f9fdea5430391c2abea0f2f90d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac3f2fb5f8c479a8e676c645ed032d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cd1fa1d19d4b80b2151d3cbf99c313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuraracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    results = {}\n",
    "    results.update(accuraracy.compute(predictions=predictions, references=labels))\n",
    "    results.update(recall.compute(predictions=predictions, references=labels, average=\"micro\"))\n",
    "    results.update(precision.compute(predictions=predictions, references=labels, average=\"micro\"))\n",
    "    results.update(f1.compute(predictions=predictions, references=labels, average=\"micro\"))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c2102ce51b4bd49ef20e5e3c7298b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2116\\4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1643\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         )\n\u001b[1;32m-> 1645\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1646\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1937\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1938\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1940\u001b[0m                 if (\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2758\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2759\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2761\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2782\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2783\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2784\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2785\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2786\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1560\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1562\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1563\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1018\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m         )\n\u001b[1;32m-> 1020\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1021\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    608\u001b[0m                 )\n\u001b[0;32m    609\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    496\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[1;32m--> 425\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    426\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linh nguyen\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[1;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True, device=\"mps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc1 = \"\"\"\n",
    "ch denke an das, was Du empfiehlst:\n",
    "\"Talent borrows, genius steals\"\n",
    "An das was Du mir anvertraust:\n",
    "\"Wähle dir deine fallen aus\"\n",
    "Ich glaub ich kanns erst jetzt verstehen\n",
    "Wir müssen durch den Spiegel gehen\n",
    "Danke auch für den Versuch\n",
    "Das hier ist kein Wörterbuch\n",
    "Du streichst mir über\n",
    "Mein Gesicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Meine Liebe\n",
    "Dein Verzicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Völker! Auf zum Gefecht!\n",
    "Die Illusion wird Menschenrecht\n",
    "Ich bin nicht allein in meiner Sucht\n",
    "Vor den Spießern auf der Flucht\n",
    "Du denkst an mich\n",
    "Ich denk' an Dich\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Unser Denken\n",
    "Verbündet sich\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Jeden Morgen, jede Nacht\n",
    "Jeden Morgen, jede Nacht\n",
    "Jeden Morgen, jede Nacht\n",
    "Jeden Morgen, jede Nacht\n",
    "Du streichst mir über\n",
    "Mein Gesicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Meine Liebe\n",
    "Dein Verzicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Du streichst mir über\n",
    "Mein Gesicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Meine Liebe\n",
    "Dein Verzicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kool1 = \"\"\"Ich komm auf die Bühne leg Hand ans Mic\n",
    "Fang langsam an und demolier dich am Ende wie Van Damme\n",
    "Geh hol deine Gun dann, wärs fair\n",
    "Doch bis dahin ist das einzge was du gewinnst nur Land man\n",
    "Guck euer Gerede macht Müde wie der Sandmann\n",
    "Ob Türke, Russe oder der Rest von Grand Cannes\n",
    "Bei jedem finden diese Tracks Anklang, aber was ist das\n",
    "Deine Bitch hat ein Loch wie'n Schuss von ner Pumpgun\n",
    "Na dann viel Fun man, ich an deiner Stelle würd\n",
    "Sie am randstehen lassen wie'n Wandschrank\n",
    "Egal kein tam-tam, wahrt den Anstand meine\n",
    "Texte ficken alles wie Matrosen auf Landgang\n",
    "Für mich ist rappen wie wandern, s läuft\n",
    "Und ich bediene gerne euer Team alleine die Bank lang\n",
    "Danke an alle die mein Talent verkannt ham\n",
    "Heute seht ihr wohl alles anders wie in nem Handstand, oder nicht?\n",
    "Nie, niemals kriegt ihr mich weg, nie wi-ieder geh' ich\n",
    "Keiner von euch kann's, wie di-ieser Typ ist da Vinci\n",
    "Und das hier ist meine Mona-Lisa\n",
    "Des Original, die Eins, vielmals von ihm kopiert\n",
    "Aber niemals erreicht, ihr meint ihr seids\n",
    "Doch kommt nicht ran an Mona-Lisa\n",
    "Ich renn' durch Beats wie'n Athlet\n",
    "Ich nehm' Wörter und Silben und kann sie im Takt dreh'n\n",
    "Des ist'n Fetisch, ähnlich wie bei Leuten, die auf Leder und Lack stehen\n",
    "Ich kann mich an meinen Texten nicht satt sehen\n",
    "Ich hab' den übelsten dich-glatt-bügelnden Flow\n",
    "Alles, was ich rappe, muss \"Zack Zack Zack\" gehen\n",
    "Sie wollen mein Album mehr, als Miss Milian nackt sehen\n",
    "Feature mit dir, ich muss ablehnen\n",
    "Was geht, alle nenn'n dich Kackface\n",
    "Als würde dein Kopf unmittelbar hinter dem Sack kleben\n",
    "Mich will jeder wie 'ne Wiese plattmähen\n",
    "Mich battlen lass', wie bei 'nem Liebesakt blähen\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(kool1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a1630c5465e12de1581cf816a14356e2922c5c54774098183d7941391ccb17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
