{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy Tutorial\n",
    "\n",
    "spaCy ist eine Python-Bibliothek für Natural Language Processing (NLP). Sie bietet eine Vielzahl von Funktionen, die für die Analyse von Texten nützlich sind. In diesem Tutorial werden wir uns mit den Grundlagen von spaCy vertraut machen und einige der Funktionen ausprobieren.\n",
    "\n",
    "spaCy besitzt sehr viele Funktionen unter anderem:\n",
    "- Tokenization\n",
    "- Text normalization, such as lowercasing, stemming/lemmatization\n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Sentence boundary detection\n",
    "- Named entity recognition and annotation\n",
    "\n",
    "Außerdem kommen die Sprachpakete von spaCy mit einigen wichtigen Daten:\n",
    "- Large vocabulary, including stopword lists\n",
    "- Token \"probabilities\"\n",
    "- Word vectors\n",
    "\n",
    "spaCy ist mit Hilfe von Cython geschrieben und daher sehr schnell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spaCy\n",
      "  Downloading spacy-3.5.2-cp38-cp38-win_amd64.whl (12.6 MB)\n",
      "     --------------------------------------- 12.6/12.6 MB 25.2 MB/s eta 0:00:00\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp38-cp38-win_amd64.whl (482 kB)\n",
      "     ------------------------------------- 482.7/482.7 kB 31.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from spaCy) (50.3.1.post20201107)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from spaCy) (20.4)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from spaCy) (2.11.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from spaCy) (1.21.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp38-cp38-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 35.0 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.6 MB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-win_amd64.whl (18 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp38-cp38-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.7/96.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from spaCy) (4.50.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from spaCy) (2.24.0)\n",
      "Requirement already satisfied: six in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from packaging>=20.0->spaCy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from packaging>=20.0->spaCy) (2.4.7)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (1.25.11)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp38-cp38-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 28.1 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spaCy) (7.1.2)\n",
      "Collecting colorama>=0.4.6\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lotus\\anaconda3\\lib\\site-packages (from jinja2->spaCy) (1.1.1)\n",
      "Installing collected packages: cymem, typing-extensions, typer, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, colorama, catalogue, blis, wasabi, srsly, pydantic, preshed, pathy, confection, thinc, spaCy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 colorama-0.4.6 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spaCy-3.5.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 typing-extensions-4.5.0 wasabi-1.1.1\n"
     ]
    }
   ],
   "source": [
    "%pip install spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Ich mag's, wenn sich die Wut entfacht\n",
    "Und ich mag deine Zaubermacht\n",
    "Ich mag die Tiere nachts im Wald\n",
    "Wenn sie flüstern, dass es schallt\n",
    "Ich mag den Weg, ich mag das Ziel\n",
    "Den Exzess, das Selbstexil\n",
    "Ich mag erschaudern und nicht zu knapp\n",
    "Ich gebe jedem etwas ab\n",
    "All das mag ich\n",
    "All das mag ich\n",
    "Aber hier leben, nein danke\n",
    "Aber hier leben, nein danke\n",
    "Ich mag die Wolken und den Wind\n",
    "Ich mag das Licht, das du mir bringst\n",
    "Wenn du dich um mich bemühst\n",
    "Wenn der Wahnsinn flammend grüßt\n",
    "Wenn die Träume Funken sprühen\n",
    "Und die weißen Blumen blühen\n",
    "Ich mag die Engel kurz vor dem Fall\n",
    "Diamanten aus dem All\n",
    "All das mag ich\n",
    "All das mag ich\n",
    "Aber hier leben, nein danke\n",
    "Aber hier leben, nein danke\n",
    "Ich mag die Spiegelung der Luft\n",
    "Und wenn die Sehnsucht nie verpufft\n",
    "Den Glanz des Lebens in einem Tag\n",
    "Ich mag den Zweifel, der an mir nagt\n",
    "Wenn meine Angst mich schnell verlässt\n",
    "Ich mag den Tanz, das Idiotenfest\n",
    "Wenn wir irren, nachts im Kreis\n",
    "Eine Bewegung gegen den Fleiß\n",
    "All das mag ich\n",
    "All das mag ich\n",
    "Aber hier leben, nein danke\n",
    "Aber hier leben, nein danke\n",
    "Aber hier leben, nein danke\n",
    "Aber hier leben, nein danke\n",
    "Aber hier leben, nein danke\n",
    "Aber hier leben, nein danke\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste von deutschen Sprachmodellen: https://spacy.io/models/de\n",
    "\n",
    "Wir laden das größte deutsche Sprachmodell (das keine dependency auf HuggingFace Transformer hat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.5.0/de_core_news_lg-3.5.0-py3-none-any.whl (567.8 MB)\n",
      "     -------------------------------------- 567.8/567.8 MB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from de-core-news-lg==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lotus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-lg==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wenden nun die spaCy pipeline auf den Text an und bekommen ein _Document_. Ein Document besteht unter anderem aus einer Liste an Tokens (tokenization), die wir in den nächsten Schritten analysieren.\n",
    "\n",
    "#### What is a Token?\n",
    "A token is a single chopped up element of the sentence, which could be a word or a group of words to analyse. The task of chopping the sentence up is called \"tokenisation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tags\n",
    "\n",
    "#### What is a Speech Tag?\n",
    "A speech tag is a context sensitive description of what a word means in the context of the whole sentence.\n",
    "More information about the kinds of speech tags which are used in NLP can be [found here](http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/).\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. CARDINAL, Cardinal Number - 1,2,3\n",
    "2. PROPN, Proper Noun, Singular - \"Matic\", \"Andraz\", \"Cardiff\"\n",
    "3. INTJ, Interjection - \"Uhhhhhhhhhhh\"\n",
    "\n",
    "- Text: The original word text.\n",
    "- Lemma: The base form of the word.\n",
    "- POS: The simple UPOS part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag.\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "- Shape: The word shape – capitalization, punctuation, digits.\n",
    "- is alpha: Is the token an alpha character?\n",
    "- is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Ich || pos: PRON || tag: PPER dep: sb || lemma: ich\n",
      "text: mag's || pos: AUX || tag: VVFIN dep: ROOT || lemma: mag's\n",
      "text: , || pos: PUNCT || tag: $, dep: punct || lemma: --\n",
      "text: wenn || pos: SCONJ || tag: KOUS dep: cp || lemma: wenn\n",
      "text: sich || pos: PRON || tag: PRF dep: oa || lemma: sich\n",
      "text: die || pos: DET || tag: ART dep: nk || lemma: der\n",
      "text: Wut || pos: NOUN || tag: NN dep: sb || lemma: Wut\n",
      "text: entfacht || pos: VERB || tag: VVPP dep: mo || lemma: entfachen\n",
      "text: \n",
      " || pos: SPACE || tag: _SP dep: dep || lemma: \n",
      "\n",
      "text: Und || pos: CCONJ || tag: KON dep: ju || lemma: und\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:10]:\n",
    "    print(f'text: {token.text} || pos: {token.pos_} || tag: {token.tag_} dep: {token.dep_} || lemma: {token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are Stop Words?\n",
    "\n",
    "Stop words are the common words in a vocabulary which are of little value when considering word frequencies in text. This is because they don't provide much useful information about what the sentence is telling the reader.\n",
    "\n",
    "Example: _\"the\",\"and\",\"a\",\"are\",\"is\"_\n",
    "\n",
    "#### What is a Corpus?\n",
    "\n",
    "A corpus (plural: corpora) is a large collection of text or documents and can provide useful training data for NLP models. A corpus might be built from transcribed speech or a collection of manuscripts. Each item in a corpus is not necessarily unique and frequency counts of words can assist in uncovering the structure in a corpus.\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Every word written in the complete works of Shakespeare\n",
    "2. Every word spoken on BBC Radio channels for the past 30 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ich', 'wenn', 'sich', 'die', 'Und', 'ich', 'mag', 'deine', 'Ich', 'mag']\n"
     ]
    }
   ],
   "source": [
    "# Get stop words\n",
    "stop_words = [token.text for token in doc if token.is_stop]\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns auch die Liste der Sätze holen. Dabei unterteilt spaCy nicht nur nach Interpunktion, sondern auch nach semantischen Eigenschaften. Gerade bei Songtexten funktioniert das aber nicht immer so gut, wie wir gleich sehen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['Wenn', 'sie', 'flüstern', ',', 'dass', 'es', 'schallt', '\\n', 'Ich', 'mag', 'den', 'Weg', ',', 'ich', 'mag', 'das', 'Ziel', '\\n', 'Den', 'Exzess', ',', 'das', 'Selbstexil', '\\n', 'Ich', 'mag', 'erschaudern', 'und', 'nicht', 'zu', 'knapp', '\\n', 'Ich', 'gebe', 'jedem', 'etwas', 'ab', '\\n', 'All', 'das', 'mag', 'ich', '\\n', 'All', 'das', 'mag', 'ich', '\\n', 'Aber', 'hier', 'leben', ',', 'nein', 'danke', '\\n', 'Aber', 'hier', 'leben', ',', 'nein', 'danke', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Get sentences\n",
    "sentences = list(doc.sents)\n",
    "print(len(sentences))\n",
    "# Hier sehen wir auch schon ein Problem mit der Tokenisierung.\n",
    "print([token.text for token in sentences[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities\n",
    "\n",
    "#### Named Entities\n",
    "\n",
    "A named entity is any real world object such as a person, location, organisation or product with a proper name. \n",
    "\n",
    "Example:\n",
    "\n",
    "\t1. Barack Obama\n",
    "\t2. Edinburgh\n",
    "\t3. Ferrari Enzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selbstexil\n",
      "Ich (MISC)\n",
      "Engel (PER)\n",
      "All\n",
      "All (MISC)\n",
      "Idiotenfest (PER)\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text} ({ent.label_})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brack Obama (PER)\n",
      "USA (LOC)\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Brack Obama war der 44. Präsident der USA.\")\n",
    "for ent in doc2.ents:\n",
    "    print(f'{ent.text} ({ent.label_})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing\n",
    "\n",
    "#### What are syntactic dependencies?\n",
    "\n",
    "We have the speech tags and we have all of the tokens in a sentence, but how do we relate the two to uncover the syntax in a sentence? Syntactic dependencies describe how each type of word relates to each other in a sentence, this is important in NLP in order to extract structure and understand grammar in plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich mag's, wenn sich die Wut entfacht\n",
      "\n",
      "mag's\n",
      "[Ich, ,, entfacht]\n"
     ]
    }
   ],
   "source": [
    "sent = sentences[0]\n",
    "print(sent.text)\n",
    "print(sent.root)\n",
    "print(list(sent.root.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit displaCy können wir den dependency tree visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"de\" id=\"c1e63487ad9f4f04b29b3f33514a3251-0\" class=\"displacy\" width=\"1450\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Ich</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">mag's,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">wenn</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sich</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">die</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Wut</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">entfacht</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">\n",
       "</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">SPACE</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c1e63487ad9f4f04b29b3f33514a3251-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c1e63487ad9f4f04b29b3f33514a3251-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">sb</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c1e63487ad9f4f04b29b3f33514a3251-0-1\" stroke-width=\"2px\" d=\"M420,352.0 C420,89.5 1095.0,89.5 1095.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c1e63487ad9f4f04b29b3f33514a3251-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c1e63487ad9f4f04b29b3f33514a3251-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 1090.0,177.0 1090.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c1e63487ad9f4f04b29b3f33514a3251-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">oa</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c1e63487ad9f4f04b29b3f33514a3251-0-3\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c1e63487ad9f4f04b29b3f33514a3251-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c1e63487ad9f4f04b29b3f33514a3251-0-4\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c1e63487ad9f4f04b29b3f33514a3251-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">sb</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c1e63487ad9f4f04b29b3f33514a3251-0-5\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 1100.0,2.0 1100.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c1e63487ad9f4f04b29b3f33514a3251-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mo</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,354.0 L1108.0,342.0 1092.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c1e63487ad9f4f04b29b3f33514a3251-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c1e63487ad9f4f04b29b3f33514a3251-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1260.0,354.0 L1268.0,342.0 1252.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Beyza\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " geht am Sonntag nach \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Düsseldorf\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " um einen Film zu schauen</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#displacy.render(sentences[1], style=\"ent\")\n",
    "displacy.render(nlp(\"Beyza geht am Sonntag nach Düsseldorf um einen Film zu schauen\"), style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding / Similarity\n",
    "\n",
    "#### What are Word embeddings?\n",
    "\n",
    "A word embedding is a representation of a word, and by extension a whole language corpus, in a vector or other form of numerical mapping. This allows words to be treated numerically with word similarity represented as spatial difference in the dimensions of the word embedding mapping.\n",
    "\n",
    "Example:\n",
    "\t\n",
    "With word embeddings we can understand that vector operations describe word similarity. This means that we can see vector proofs of statements such as:\n",
    "\n",
    "\tking-queen==man-woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8160741925239563\n",
      "0.7664776542293525\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"Ich mag Orangen und Mandarinen.\")\n",
    "doc4 = nlp(\"Ich hasse Äpfel und Bananen\")\n",
    "orangen = doc3[2]\n",
    "birnen = doc3[4]\n",
    "print(orangen.similarity(birnen))\n",
    "print(doc3.similarity(doc4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Pipelines sind eine Möglichkeit, spaCy mit verschiedenen Funktionen zu erweitern. Wenn wir ein Sprachpaket laden, bekommen wir eine Standard-Pipeline, die aus Tokenizer, Tagger, Parser, NER und Entity Linker besteht. Wir können diese Pipeline aber auch erweitern, indem wir weitere Funktionen hinzufügen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytextrank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-cc2479437ca0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytextrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# add PyTextRank to the spaCy pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# https://derwen.ai/docs/ptr/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"textrank\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytextrank'"
     ]
    }
   ],
   "source": [
    "import pytextrank\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "# https://derwen.ai/docs/ptr/\n",
    "nlp.add_pipe(\"textrank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funken\n",
      "0.06960498892566462 1\n",
      "[Funken]\n",
      "All\n",
      "0.0681919105236853 1\n",
      "[All]\n",
      "Kreis\n",
      "0.05687523895429876 1\n",
      "[Kreis]\n",
      "Idiotenfest\n",
      "0.04853098558702389 1\n",
      "[Idiotenfest]\n",
      "Wald\n",
      "0.04693971377563045 1\n",
      "[Wald]\n",
      "Diamanten\n",
      "0.04428881417751234 1\n",
      "[Diamanten]\n",
      "Engel\n",
      "0.03451061082711543 1\n",
      "[Engel]\n",
      "den Fleiß\n",
      "0.029186060964137752 1\n",
      "[den Fleiß]\n",
      "die Träume\n",
      "0.029120810505937896 1\n",
      "[die Träume]\n",
      "das Selbstexil\n",
      "0.028291180429706193 1\n",
      "[das Selbstexil]\n",
      "Den Exzess\n",
      "0.027032795649409438 1\n",
      "[Den Exzess]\n",
      "die weißen Blumen\n",
      "0.02687071711994295 1\n",
      "[die weißen Blumen]\n",
      "den Zweifel\n",
      "0.02680582991934769 1\n",
      "[den Zweifel]\n",
      "der Wahnsinn\n",
      "0.026451344456483436 1\n",
      "[der Wahnsinn]\n",
      "das Ziel\n",
      "0.025326207184185347 1\n",
      "[das Ziel]\n",
      "einem Tag\n",
      "0.024843241373835395 1\n",
      "[einem Tag]\n",
      "den Weg\n",
      "0.02357745637165401 1\n",
      "[den Weg]\n",
      "dem All\n",
      "0.02317363922321631 1\n",
      "[dem All]\n",
      "des Lebens\n",
      "0.02317363922321631 1\n",
      "[des Lebens]\n",
      "das Idiotenfest\n",
      "0.022415502074164873 1\n",
      "[das Idiotenfest]\n",
      "der Luft\n",
      "0.02168052510723538 1\n",
      "[der Luft]\n",
      "Selbstexil\n",
      "Ich\n",
      "0.020544613246650238 1\n",
      "[Selbstexil\n",
      "Ich]\n",
      "Den Glanz\n",
      "0.020456127029980856 1\n",
      "[Den Glanz]\n",
      "das Licht\n",
      "0.020456127029980856 1\n",
      "[das Licht]\n",
      "deine Zaubermacht\n",
      "0.01978685248554706 1\n",
      "[deine Zaubermacht]\n",
      "den Tanz\n",
      "0.01978685248554706 1\n",
      "[den Tanz]\n",
      "dem Fall\n",
      "0.018057293999138718 1\n",
      "[dem Fall]\n",
      "den Wind\n",
      "0.018057293999138718 1\n",
      "[den Wind]\n",
      "All\n",
      "All\n",
      "0.016828334771725455 1\n",
      "[All\n",
      "All]\n",
      "die Engel\n",
      "0.015939768360746804 1\n",
      "[die Engel]\n",
      "die Sehnsucht\n",
      "0.015939768360746804 1\n",
      "[die Sehnsucht]\n",
      "die Spiegelung\n",
      "0.015939768360746804 1\n",
      "[die Spiegelung]\n",
      "die Tiere\n",
      "0.015939768360746804 1\n",
      "[die Tiere]\n",
      "die Wolken\n",
      "0.015939768360746804 1\n",
      "[die Wolken]\n",
      "die Wut\n",
      "0.015939768360746804 1\n",
      "[die Wut]\n",
      "meine Angst\n",
      "0.015939768360746804 1\n",
      "[meine Angst]\n",
      "All das\n",
      "0.0 3\n",
      "[All das, All das, All das]\n",
      "Ich\n",
      "0.0 11\n",
      "[Ich, Ich, Ich, Ich, Ich, Ich, Ich, Ich, Ich, Ich, Ich]\n",
      "das\n",
      "0.0 4\n",
      "[das, das, das, das]\n",
      "der\n",
      "0.0 1\n",
      "[der]\n",
      "dich\n",
      "0.0 1\n",
      "[dich]\n",
      "du\n",
      "0.0 2\n",
      "[du, du]\n",
      "es\n",
      "0.0 1\n",
      "[es]\n",
      "ich\n",
      "0.0 8\n",
      "[ich, ich, ich, ich, ich, ich, ich, ich]\n",
      "jedem\n",
      "0.0 1\n",
      "[jedem]\n",
      "mich\n",
      "0.0 2\n",
      "[mich, mich]\n",
      "mir\n",
      "0.0 2\n",
      "[mir, mir]\n",
      "sich\n",
      "0.0 1\n",
      "[sich]\n",
      "sie\n",
      "0.0 1\n",
      "[sie]\n",
      "wir\n",
      "0.0 1\n",
      "[wir]\n"
     ]
    }
   ],
   "source": [
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy_language_detection import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_language_detection.spacy_language_detector.LanguageDetector at 0x28caa23b0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector(seed=42)  # We use the seed 42\n",
    "\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'de', 'score': 0.9999962758266623}\n",
      "{'language': 'de', 'score': 0.9999968689008364}\n",
      "{'language': 'de', 'score': 0.9999972447608682}\n",
      "{'language': 'de', 'score': 0.999996663073208}\n",
      "{'language': 'de', 'score': 0.9999960900081714}\n",
      "{'language': 'de', 'score': 0.9999961437337193}\n",
      "{'language': 'de', 'score': 0.9999959487828781}\n",
      "{'language': 'de', 'score': 0.9999957419936483}\n",
      "{'language': 'de', 'score': 0.999996150605792}\n",
      "{'language': 'de', 'score': 0.9999959818918924}\n",
      "{'language': 'de', 'score': 0.9999945277750812}\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mFailed to start the Kernel '.venv (Python 3.10.10)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Canceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "**Das profanity Paket ist archived und unterstützt nicht mehr spaCy 3...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.to_disk(\"custom_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a1630c5465e12de1581cf816a14356e2922c5c54774098183d7941391ccb17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
